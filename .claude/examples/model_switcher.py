#!/usr/bin/env python3
"""
AI æ¨¡å‹åˆ‡æ¢ä½¿ç”¨ç¤ºä¾‹
æ”¯æŒæœ¬åœ° cursorweb2api çš„ 24 ä¸ªæ¨¡å‹
"""

import requests
import json

class MultiModelAI:
    def __init__(self, base_url="http://localhost:8000/v1", api_key="aaa"):
        self.base_url = base_url
        self.api_key = api_key

    def chat(self, model, message, **kwargs):
        """
        è°ƒç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œå¯¹è¯

        Args:
            model: æ¨¡å‹IDï¼ˆå¦‚ "claude-4.5-sonnet", "gpt-4o" ç­‰ï¼‰
            message: ç”¨æˆ·æ¶ˆæ¯
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰
        """
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        data = {
            "model": model,
            "messages": [{"role": "user", "content": message}],
            **kwargs
        }

        response = requests.post(url, headers=headers, json=data)
        result = response.json()

        return result["choices"][0]["message"]["content"]

    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        url = f"{self.base_url}/models"
        headers = {"Authorization": f"Bearer {self.api_key}"}

        response = requests.get(url, headers=headers)
        models = response.json()

        return [model["id"] for model in models["data"]]


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    ai = MultiModelAI()

    # 1. æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
    print("ğŸ“‹ å¯ç”¨æ¨¡å‹:")
    models = ai.list_models()
    for i, model in enumerate(models, 1):
        print(f"  {i}. {model}")

    print("\n" + "="*50 + "\n")

    # 2. ä½¿ç”¨ Claude 4.5 Sonnet
    print("ğŸ¤– Claude 4.5 Sonnet:")
    response = ai.chat("claude-4.5-sonnet", "ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
    print(f"   {response}\n")

    # 3. ä½¿ç”¨ GPT-4o
    print("ğŸ¤– GPT-4o:")
    response = ai.chat("gpt-4o", "ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
    print(f"   {response}\n")

    # 4. ä½¿ç”¨ Gemini 2.5 Flashï¼ˆå¿«é€Ÿï¼‰
    print("ğŸ¤– Gemini 2.5 Flash:")
    response = ai.chat("gemini-2.5-flash", "ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
    print(f"   {response}\n")

    # 5. ä½¿ç”¨ DeepSeek R1ï¼ˆæ¨ç†ï¼‰
    print("ğŸ¤– DeepSeek R1 (æ¨ç†):")
    response = ai.chat("deepseek-r1", "1+1=?", temperature=0.1)
    print(f"   {response}\n")

    # 6. ä½¿ç”¨ Code Supernovaï¼ˆç¼–ç¨‹ï¼‰
    print("ğŸ¤– Code Supernova (ç¼–ç¨‹):")
    response = ai.chat("code-supernova-1-million", "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºå‡½æ•°", max_tokens=500)
    print(f"   {response}\n")

    # 7. å¯¹æ¯”ä¸åŒæ¨¡å‹çš„å›ç­”
    print("\n" + "="*50)
    print("ğŸ“Š å¯¹æ¯”æµ‹è¯•ï¼šåŒä¸€ä¸ªé—®é¢˜ç”¨ä¸åŒæ¨¡å‹")
    print("="*50 + "\n")

    question = "ä»€ä¹ˆæ˜¯é‡å­çº ç¼ ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Šã€‚"
    test_models = ["claude-4.5-sonnet", "gpt-4o", "gemini-2.5-flash", "deepseek-v3.1"]

    for model in test_models:
        print(f"ğŸ¤– {model}:")
        response = ai.chat(model, question, max_tokens=100)
        print(f"   {response}\n")
